{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/5W2faCcS3DMWASsl1/ij",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SusanLL/CS4375_HW/blob/main/CS_4375_HW2_FFNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FFNN Model**"
      ],
      "metadata": {
        "id": "zynZP-mhb0mN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from argparse import ArgumentParser"
      ],
      "metadata": {
        "id": "De7E1NU7uEF9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unk = '<UNK>'\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, input_dim, h):\n",
        "        super(FFNN, self).__init__()\n",
        "        self.h = h\n",
        "        self.W1 = nn.Linear(input_dim, h)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.output_dim = 5\n",
        "        self.W2 = nn.Linear(h, self.output_dim)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        self.loss = nn.NLLLoss()\n",
        "\n",
        "    def compute_Loss(self, predicted_vector, gold_label):\n",
        "        return self.loss(predicted_vector, gold_label)\n",
        "\n",
        "    def forward(self, input_vector):\n",
        "       # Ensure input_vector has a batch dimension\n",
        "        if input_vector.dim() == 1:\n",
        "          input_vector = input_vector.unsqueeze(0)\n",
        "        # obtain first hidden layer representation\n",
        "        hidden_rep = self.activation(self.W1(input_vector))\n",
        "\n",
        "        # obtain output layer representation\n",
        "        z = self.W2(hidden_rep)\n",
        "\n",
        "        # obtain probability dist.\n",
        "        predicted_vector = self.softmax(z)\n",
        "\n",
        "        return predicted_vector"
      ],
      "metadata": {
        "id": "2ZAYPhgRa017"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns:\n",
        "# vocab = A set of strings corresponding to the vocabulary\n",
        "def make_vocab(data):\n",
        "    vocab = set()\n",
        "    for document, _ in data:\n",
        "        for word in document:\n",
        "            vocab.add(word)\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "rP3pkO6Uz8Qy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns:\n",
        "# vocab = A set of strings corresponding to the vocabulary including <UNK>\n",
        "# word2index = A dictionary mapping word/token to its index (a number in 0, ..., V - 1)\n",
        "# index2word = A dictionary inverting the mapping of word2index\n",
        "def make_indices(vocab):\n",
        "    vocab_list = sorted(vocab)\n",
        "    vocab_list.append(unk)\n",
        "    word2index = {}\n",
        "    index2word = {}\n",
        "    for index, word in enumerate(vocab_list):\n",
        "        word2index[word] = index\n",
        "        index2word[index] = word\n",
        "    vocab.add(unk)\n",
        "    return vocab, word2index, index2word"
      ],
      "metadata": {
        "id": "c_An4sPe0BSX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns:\n",
        "# vectorized_data = A list of pairs (vector representation of input, y)\n",
        "def convert_to_vector_representation(data, word2index):\n",
        "    vectorized_data = []\n",
        "    for document, y in data:\n",
        "        vector = torch.zeros(len(word2index))\n",
        "        for word in document:\n",
        "            index = word2index.get(word, word2index[unk])\n",
        "            vector[index] += 1\n",
        "        vectorized_data.append((vector, y))\n",
        "    return vectorized_data"
      ],
      "metadata": {
        "id": "1zi21Vww0FUv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(train_data, val_data):\n",
        "    with open(train_data) as training_f:\n",
        "        training = json.load(training_f)\n",
        "    with open(val_data) as valid_f:\n",
        "        validation = json.load(valid_f)\n",
        "\n",
        "    tra = []\n",
        "    val = []\n",
        "    for elt in training:\n",
        "        tra.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n",
        "    for elt in validation:\n",
        "        val.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n",
        "\n",
        "    return tra, val"
      ],
      "metadata": {
        "id": "PCxmqOMj0Kg_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz5mdsUusE3c",
        "outputId": "89b4a787-05a0-4890-be73-4561b6f096df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== Loading data ==========\n",
            "========== Vectorizing data ==========\n",
            "Training and Validation Results\n",
            "==============================\n",
            "========== Training for 10 epochs ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:17<00:00, 28.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Training accuracy: 0.5242, Training time: 17.36 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 122.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Validation accuracy: 0.5413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:19<00:00, 26.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Training accuracy: 0.5766, Training time: 19.15 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 119.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Validation accuracy: 0.5763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:17<00:00, 28.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Training accuracy: 0.6041, Training time: 17.36 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 132.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Validation accuracy: 0.5687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:17<00:00, 27.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Training accuracy: 0.6356, Training time: 17.96 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 98.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Validation accuracy: 0.5875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:18<00:00, 27.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Training accuracy: 0.6502, Training time: 18.42 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 124.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Validation accuracy: 0.5813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:18<00:00, 27.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 - Training accuracy: 0.6565, Training time: 18.31 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 132.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 - Validation accuracy: 0.6025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:18<00:00, 27.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 - Training accuracy: 0.6996, Training time: 18.09 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 93.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 - Validation accuracy: 0.5437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:16<00:00, 29.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 - Training accuracy: 0.6999, Training time: 16.80 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 133.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 - Validation accuracy: 0.5962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:16<00:00, 30.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 - Training accuracy: 0.7255, Training time: 16.58 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 139.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 - Validation accuracy: 0.6200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:16<00:00, 30.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 - Training accuracy: 0.7412, Training time: 16.59 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 136.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 - Validation accuracy: 0.6062\n",
            "Training and validation complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from argparse import ArgumentParser\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import torch\n",
        "import time\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = ArgumentParser()\n",
        "    parser.add_argument(\"-hd\", \"--hidden_dim\", type=int, required=True, help=\"hidden_dim\")\n",
        "    parser.add_argument(\"-e\", \"--epochs\", type=int, required=True, help=\"num of epochs to train\")\n",
        "    parser.add_argument(\"--train_data\", required=True, help=\"/content/FFNN/training.json\")\n",
        "    parser.add_argument(\"--val_data\", required=True, help=\"/content/FFNN/validation.json\")\n",
        "    parser.add_argument(\"--test_data\", default=\"to fill\", help=\"/content/FFNN/test.json\")\n",
        "    parser.add_argument('--do_train', action='store_true')\n",
        "\n",
        "    arg_list = [\n",
        "        \"--hidden_dim\", \"16\",\n",
        "        \"--epochs\", \"10\",\n",
        "        \"--train_data\", \"/content/FFNN/training.json\",\n",
        "        \"--val_data\", \"/content/FFNN/validation.json\"\n",
        "    ]\n",
        "    args = parser.parse_args(arg_list)\n",
        "\n",
        "    # Fix random seeds\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Load data\n",
        "    print(\"========== Loading data ==========\")\n",
        "    train_data, valid_data = load_data(args.train_data, args.val_data)\n",
        "    vocab = make_vocab(train_data)\n",
        "    vocab, word2index, index2word = make_indices(vocab)\n",
        "\n",
        "    print(\"========== Vectorizing data ==========\")\n",
        "    train_data = convert_to_vector_representation(train_data, word2index)\n",
        "    valid_data = convert_to_vector_representation(valid_data, word2index)\n",
        "\n",
        "    model = FFNN(input_dim=len(vocab), h=args.hidden_dim)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-5)\n",
        "\n",
        "    print(\"Training and Validation Results\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    print(\"========== Training for {} epochs ==========\".format(args.epochs))\n",
        "    for epoch in range(args.epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        start_time = time.time()\n",
        "        random.shuffle(train_data)\n",
        "        minibatch_size = 16\n",
        "        N = len(train_data)\n",
        "\n",
        "        for minibatch_index in tqdm(range(N // minibatch_size)):\n",
        "            optimizer.zero_grad()\n",
        "            loss = None\n",
        "            for example_index in range(minibatch_size):\n",
        "                input_vector, gold_label = train_data[minibatch_index * minibatch_size + example_index]\n",
        "                predicted_vector = model(input_vector)\n",
        "                predicted_label = torch.argmax(predicted_vector)\n",
        "                correct += int(predicted_label == gold_label)\n",
        "                total += 1\n",
        "                example_loss = model.compute_Loss(predicted_vector.view(1, -1), torch.tensor([gold_label]))\n",
        "                if loss is None:\n",
        "                    loss = example_loss\n",
        "                else:\n",
        "                    loss += example_loss\n",
        "            loss = loss / minibatch_size\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        training_accuracy = correct / total\n",
        "        epoch_time = time.time() - start_time\n",
        "        print(f\"Epoch {epoch + 1} - Training accuracy: {training_accuracy:.4f}, Training time: {epoch_time:.2f} seconds\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for minibatch_index in tqdm(range(len(valid_data) // minibatch_size)):\n",
        "                loss = None\n",
        "                for example_index in range(minibatch_size):\n",
        "                    input_vector, gold_label = valid_data[minibatch_index * minibatch_size + example_index]\n",
        "                    predicted_vector = model(input_vector)\n",
        "                    predicted_label = torch.argmax(predicted_vector)\n",
        "                    correct += int(predicted_label == gold_label)\n",
        "                    total += 1\n",
        "                    example_loss = model.compute_Loss(predicted_vector.view(1, -1), torch.tensor([gold_label]))\n",
        "                    if loss is None:\n",
        "                        loss = example_loss\n",
        "                    else:\n",
        "                        loss += example_loss\n",
        "                loss = loss / minibatch_size\n",
        "\n",
        "        validation_accuracy = correct / total\n",
        "        print(f\"Epoch {epoch + 1} - Validation accuracy: {validation_accuracy:.4f}\")\n",
        "\n",
        "    print(\"Training and validation complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load JSON data from the specified file path.\"\"\"\n",
        "    # Construct the full file path using os.path.join\n",
        "    full_path = os.path.join('/content/FFNN', file_path)\n",
        "    with open(full_path, 'r') as f:  # Open the file using the constructed path\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def calculate_statistics(data):\n",
        "    \"\"\"Calculate number of examples, average words per review, and star rating distribution.\"\"\"\n",
        "    num_examples = len(data)\n",
        "    word_counts = [len(review['text'].split()) for review in data]\n",
        "    avg_words_per_review = np.mean(word_counts)\n",
        "\n",
        "    # Count star ratings distribution\n",
        "    star_ratings = [int(review['stars']) for review in data]\n",
        "    unique, counts = np.unique(star_ratings, return_counts=True)\n",
        "    star_distribution = dict(zip(unique, counts))\n",
        "\n",
        "    # Convert star distribution to percentages\n",
        "    star_distribution_percentage = {star: (count / num_examples) * 100 for star, count in star_distribution.items()}\n",
        "\n",
        "    return num_examples, avg_words_per_review, star_distribution_percentage\n",
        "\n",
        "# Load each dataset and calculate statistics\n",
        "datasets = {\n",
        "    \"Training\": \"training.json\",\n",
        "    \"Validation\": \"validation.json\",\n",
        "    \"Test\": \"test.json\"\n",
        "}\n",
        "\n",
        "for dataset_name, file_path in datasets.items():\n",
        "    data = load_data(file_path)\n",
        "    num_examples, avg_words, star_distribution = calculate_statistics(data)\n",
        "\n",
        "    # Print out the statistics\n",
        "    print(f\"{dataset_name} Set:\")\n",
        "    print(f\"  Number of Examples: {num_examples}\")\n",
        "    print(f\"  Average Words per Review: {avg_words:.2f}\")\n",
        "    print(f\"  Star Ratings Distribution: {star_distribution}\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO7Zdsf4dBCf",
        "outputId": "1825453f-523c-4a58-f43e-0a884082d8ae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set:\n",
            "  Number of Examples: 8000\n",
            "  Average Words per Review: 141.25\n",
            "  Star Ratings Distribution: {1: 40.0, 2: 40.0, 3: 20.0}\n",
            "----------------------------------------\n",
            "Validation Set:\n",
            "  Number of Examples: 800\n",
            "  Average Words per Review: 140.37\n",
            "  Star Ratings Distribution: {1: 40.0, 2: 40.0, 3: 20.0}\n",
            "----------------------------------------\n",
            "Test Set:\n",
            "  Number of Examples: 800\n",
            "  Average Words per Review: 109.77\n",
            "  Star Ratings Distribution: {3: 20.0, 4: 40.0, 5: 40.0}\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ]
}