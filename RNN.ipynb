{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/BpZLwfi9fpW72mH2buoZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SusanLL/CS4375_HW/blob/main/CS_4375_HW2_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RNN Model**"
      ],
      "metadata": {
        "id": "s0F9_lXjb7hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import string\n",
        "from argparse import ArgumentParser\n",
        "import pickle"
      ],
      "metadata": {
        "id": "1ARnhpgBkGVA"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unk = '<UNK>'\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, h):  # Add relevant parameters\n",
        "        super(RNN, self).__init__()\n",
        "        self.h = h\n",
        "        self.numOfLayer = 1\n",
        "        self.rnn = nn.RNN(input_dim, h, self.numOfLayer, nonlinearity='tanh')\n",
        "        self.W = nn.Linear(h, 5)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        self.loss = nn.NLLLoss()\n",
        "\n",
        "    def compute_Loss(self, predicted_vector, gold_label):\n",
        "        return self.loss(predicted_vector, gold_label)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "      # Step 1: Pass inputs through the RNN layer to obtain hidden layer representation\n",
        "      _, hidden = self.rnn(inputs)  # Obtain the final hidden state from the RNN\n",
        "\n",
        "      # Step 2: Pass the hidden state through a linear layer to obtain output layer representations\n",
        "      z = self.W(hidden[-1])  # Use the final hidden state for prediction\n",
        "\n",
        "      # Step 3: Obtain probability distribution over classes\n",
        "      predicted_vector = self.softmax(z)\n",
        "\n",
        "      return predicted_vector"
      ],
      "metadata": {
        "id": "jH8J1ybGkMB2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def load_data(train_data_path, val_data_path):  # Add val_data_path as an argument\n",
        "    \"\"\"Load JSON data from the specified training and validation file paths.\"\"\"\n",
        "    with open(train_data_path) as training_f:\n",
        "        training = json.load(training_f)\n",
        "    with open(val_data_path) as valid_f:  # Open and load the validation data\n",
        "        validation = json.load(valid_f)\n",
        "\n",
        "    tra = [(elt[\"text\"].split(), int(elt[\"stars\"] - 1)) for elt in training]\n",
        "    val = [(elt[\"text\"].split(), int(elt[\"stars\"] - 1)) for elt in validation]  # Process validation data\n",
        "    return tra, val  # Return both training and validation data"
      ],
      "metadata": {
        "id": "dCHBVJzEkOOg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Create a random word embedding dictionary\n",
        "vocab = [\"<UNK>\", \"example\", \"words\", \"here\"]\n",
        "word_embedding = {word: np.random.rand(50) for word in vocab}\n",
        "\n",
        "# Save to pickle file\n",
        "with open('word_embedding.pkl', 'wb') as f:\n",
        "    pickle.dump(word_embedding, f)"
      ],
      "metadata": {
        "id": "RWpicRo2ZmJX"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "try:\n",
        "    with open('word_embedding.pkl', 'rb') as f:\n",
        "        word_embedding = pickle.load(f)\n",
        "    print(\"File loaded successfully.\")\n",
        "except pickle.UnpicklingError:\n",
        "    print(\"UnpicklingError: The file is not a valid pickle file.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBFIkTvjp8I6",
        "outputId": "f20b5ec5-09b3-40a6-b5be-6ffdc8efd6a6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables to store results\n",
        "results = {\"training\": [], \"validation\": []}\n",
        "best_validation_accuracy = 0.0\n",
        "no_improvement_epochs = 0  # Counter for early stopping\n",
        "max_no_improvement = 3  # Stop after 3 epochs with no improvement\n",
        "epoch = 0\n",
        "\n",
        "while epoch < args.epochs:\n",
        "    random.shuffle(train_data)\n",
        "    model.train()\n",
        "    print(f\"Training started for epoch {epoch + 1}\")\n",
        "\n",
        "    # Training metrics\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    minibatch_size = 32\n",
        "    N = len(train_data)\n",
        "\n",
        "    loss_total = 0\n",
        "    loss_count = 0\n",
        "\n",
        "    # Training loop\n",
        "    for minibatch_index in tqdm(range(N // minibatch_size)):\n",
        "        optimizer.zero_grad()\n",
        "        loss = None\n",
        "        for example_index in range(minibatch_size):\n",
        "            input_words, gold_label = train_data[minibatch_index * minibatch_size + example_index]\n",
        "            input_words = \" \".join(input_words).translate(str.maketrans(\"\", \"\", string.punctuation)).split()\n",
        "\n",
        "            # Look up word embeddings\n",
        "            vectors = [torch.tensor(word_embedding[i.lower()], dtype=torch.float32) if i.lower() in word_embedding else torch.tensor(word_embedding[unk], dtype=torch.float32) for i in input_words]\n",
        "            vectors = torch.stack(vectors).view(len(vectors), 1, -1).float()\n",
        "            output = model(vectors)\n",
        "\n",
        "            # Calculate loss\n",
        "            example_loss = model.compute_Loss(output.view(1, -1), torch.tensor([gold_label]))\n",
        "            predicted_label = torch.argmax(output)\n",
        "            correct += int(predicted_label == gold_label)\n",
        "            total += 1\n",
        "\n",
        "            if loss is None:\n",
        "                loss = example_loss\n",
        "            else:\n",
        "                loss += example_loss\n",
        "\n",
        "        loss = loss / minibatch_size\n",
        "        loss_total += loss.data\n",
        "        loss_count += 1\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    training_accuracy = correct / total\n",
        "    results[\"training\"].append((epoch + 1, training_accuracy, loss_total / loss_count))\n",
        "    print(f\"Training accuracy for epoch {epoch + 1}: {training_accuracy:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    print(f\"Validation started for epoch {epoch + 1}\")\n",
        "    for input_words, gold_label in tqdm(valid_data):\n",
        "        input_words = \" \".join(input_words).translate(str.maketrans(\"\", \"\", string.punctuation)).split()\n",
        "        vectors = [torch.tensor(word_embedding[i.lower()], dtype=torch.float32) if i.lower() in word_embedding else torch.tensor(word_embedding[unk], dtype=torch.float32) for i in input_words]\n",
        "        vectors = torch.stack(vectors).view(len(vectors), 1, -1).float()\n",
        "        output = model(vectors)\n",
        "        predicted_label = torch.argmax(output)\n",
        "        correct += int(predicted_label == gold_label)\n",
        "        total += 1\n",
        "\n",
        "    validation_accuracy = correct / total\n",
        "    results[\"validation\"].append((epoch + 1, validation_accuracy))\n",
        "    print(f\"Validation accuracy for epoch {epoch + 1}: {validation_accuracy:.4f}\")\n",
        "\n",
        "    # Early stopping condition\n",
        "    if validation_accuracy > best_validation_accuracy:\n",
        "        best_validation_accuracy = validation_accuracy\n",
        "        no_improvement_epochs = 0  # Reset counter if improvement occurs\n",
        "    else:\n",
        "        no_improvement_epochs += 1\n",
        "\n",
        "    if no_improvement_epochs >= max_no_improvement:\n",
        "        print(f\"Early stopping triggered after {no_improvement_epochs} epochs with no improvement.\")\n",
        "        break\n",
        "\n",
        "    epoch += 1\n",
        "\n",
        "# Print final results\n",
        "print(\"\\nTraining and Validation Results\")\n",
        "print(\"=\" * 30)\n",
        "for epoch, train_acc, train_loss in results[\"training\"]:\n",
        "    print(f\"Epoch {epoch}: Training Accuracy = {train_acc:.4f}, Loss = {train_loss:.4f}\")\n",
        "for epoch, val_acc in results[\"validation\"]:\n",
        "    print(f\"Epoch {epoch}: Validation Accuracy = {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bULoIWh8tRQr",
        "outputId": "e71b7ff4-a5e5-43ab-f030-ea5ae1a03e4c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started for epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [04:45<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy for epoch 1: 0.2044\n",
            "Validation started for epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:07<00:00, 104.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy for epoch 1: 0.0000\n",
            "Training started for epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [04:37<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy for epoch 2: 0.1964\n",
            "Validation started for epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:07<00:00, 102.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy for epoch 2: 0.2000\n",
            "Training started for epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [04:43<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy for epoch 3: 0.1971\n",
            "Validation started for epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:06<00:00, 123.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy for epoch 3: 0.4000\n",
            "Training started for epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [04:38<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy for epoch 4: 0.1994\n",
            "Validation started for epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:07<00:00, 107.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy for epoch 4: 0.4000\n",
            "Training started for epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [04:35<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy for epoch 5: 0.2021\n",
            "Validation started for epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:07<00:00, 106.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy for epoch 5: 0.0000\n",
            "Training started for epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [04:31<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy for epoch 6: 0.2057\n",
            "Validation started for epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:06<00:00, 122.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy for epoch 6: 0.0000\n",
            "Early stopping triggered after 3 epochs with no improvement.\n",
            "\n",
            "Training and Validation Results\n",
            "==============================\n",
            "Epoch 1: Training Accuracy = 0.2044, Loss = 1.6915\n",
            "Epoch 2: Training Accuracy = 0.1964, Loss = 1.6998\n",
            "Epoch 3: Training Accuracy = 0.1971, Loss = 1.6967\n",
            "Epoch 4: Training Accuracy = 0.1994, Loss = 1.6900\n",
            "Epoch 5: Training Accuracy = 0.2021, Loss = 1.6904\n",
            "Epoch 6: Training Accuracy = 0.2057, Loss = 1.7023\n",
            "Epoch 1: Validation Accuracy = 0.0000\n",
            "Epoch 2: Validation Accuracy = 0.2000\n",
            "Epoch 3: Validation Accuracy = 0.4000\n",
            "Epoch 4: Validation Accuracy = 0.4000\n",
            "Epoch 5: Validation Accuracy = 0.0000\n",
            "Epoch 6: Validation Accuracy = 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load JSON data from the specified file path.\"\"\"\n",
        "    # Construct the full file path using os.path.join\n",
        "    full_path = os.path.join('/content/RNN', file_path)\n",
        "    with open(full_path, 'r') as f:  # Open the file using the constructed path\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def calculate_statistics(data):\n",
        "    \"\"\"Calculate number of examples, average words per review, and star rating distribution.\"\"\"\n",
        "    num_examples = len(data)\n",
        "    word_counts = [len(review['text'].split()) for review in data]\n",
        "    avg_words_per_review = np.mean(word_counts)\n",
        "\n",
        "    # Count star ratings distribution\n",
        "    star_ratings = [int(review['stars']) for review in data]\n",
        "    unique, counts = np.unique(star_ratings, return_counts=True)\n",
        "    star_distribution = dict(zip(unique, counts))\n",
        "\n",
        "    # Convert star distribution to percentages\n",
        "    star_distribution_percentage = {star: (count / num_examples) * 100 for star, count in star_distribution.items()}\n",
        "\n",
        "    return num_examples, avg_words_per_review, star_distribution_percentage\n",
        "\n",
        "# Load each dataset and calculate statistics\n",
        "datasets = {\n",
        "    \"Training\": \"training.json\",\n",
        "    \"Validation\": \"validation.json\",\n",
        "    \"Test\": \"test.json\"\n",
        "}\n",
        "\n",
        "for dataset_name, file_path in datasets.items():\n",
        "    data = load_data(file_path)\n",
        "    num_examples, avg_words, star_distribution = calculate_statistics(data)\n",
        "\n",
        "    # Print out the statistics\n",
        "    print(f\"{dataset_name} Set:\")\n",
        "    print(f\"  Number of Examples: {num_examples}\")\n",
        "    print(f\"  Average Words per Review: {avg_words:.2f}\")\n",
        "    print(f\"  Star Ratings Distribution: {star_distribution}\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nabVHUZvOfgg",
        "outputId": "21e2633e-c2a4-459c-b2db-db00250cfa6d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set:\n",
            "  Number of Examples: 16000\n",
            "  Average Words per Review: 124.69\n",
            "  Star Ratings Distribution: {1: 20.0, 2: 20.0, 3: 20.0, 4: 20.0, 5: 20.0}\n",
            "----------------------------------------\n",
            "Validation Set:\n",
            "  Number of Examples: 800\n",
            "  Average Words per Review: 140.37\n",
            "  Star Ratings Distribution: {1: 40.0, 2: 40.0, 3: 20.0}\n",
            "----------------------------------------\n",
            "Test Set:\n",
            "  Number of Examples: 800\n",
            "  Average Words per Review: 109.77\n",
            "  Star Ratings Distribution: {3: 20.0, 4: 40.0, 5: 40.0}\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ]
}
